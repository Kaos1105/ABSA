{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6f332e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark==3.5.1 in ./.conda/lib/python3.11/site-packages (3.5.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in ./.conda/lib/python3.11/site-packages (from pyspark==3.5.1) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in ./.conda/lib/python3.11/site-packages (2.3.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in ./.conda/lib/python3.11/site-packages (from pandas) (2.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.conda/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.conda/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.conda/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: findspark in ./.conda/lib/python3.11/site-packages (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: joblib in ./.conda/lib/python3.11/site-packages (1.5.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in ./.conda/lib/python3.11/site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in ./.conda/lib/python3.11/site-packages (from scikit-learn) (2.3.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in ./.conda/lib/python3.11/site-packages (from scikit-learn) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.conda/lib/python3.11/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.conda/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: xgboost in ./.conda/lib/python3.11/site-packages (3.0.5)\n",
      "Requirement already satisfied: numpy in ./.conda/lib/python3.11/site-packages (from xgboost) (2.3.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in ./.conda/lib/python3.11/site-packages (from xgboost) (2.28.3)\n",
      "Requirement already satisfied: scipy in ./.conda/lib/python3.11/site-packages (from xgboost) (1.16.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting pyarrow\n",
      "  Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Downloading pyarrow-21.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (42.8 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyarrow\n",
      "Successfully installed pyarrow-21.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark==3.5.1\n",
    "%pip install pandas\n",
    "%pip install findspark\n",
    "%pip install joblib\n",
    "%pip install scikit-learn\n",
    "%pip install xgboost\n",
    "%pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7625c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# PATH definitions\n",
    "DATA_DIR = \"./dataset\"  # ƒë·ªïi n·∫øu d√πng Drive\n",
    "MODEL_DIR = \"./models\"\n",
    "TRAIN_PATH = os.path.join(DATA_DIR, \"train_data.csv\")\n",
    "VAL_PATH   = os.path.join(DATA_DIR, \"val_data.csv\")\n",
    "TEST_PATH  = os.path.join(DATA_DIR, \"test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16759867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns\n",
    "TEXT_COL = \"Review\"\n",
    "ASPECTS  = [\"Price\",\"Shipping\",\"Outlook\",\"Quality\",\"Size\",\"Shop_Service\",\"General\",\"Others\"]\n",
    "\n",
    "# Label mapping\n",
    "SENT_ID2NAME = {-1: \"None\", 0: \"Negative\", 1: \"Positive\", 2: \"Neutral\"}\n",
    "LABEL_VALUES = list(SENT_ID2NAME.keys())  # [-1, 0, 1, 2]\n",
    "LABEL_NAMES  = list(SENT_ID2NAME.values())  # [\"None\",\"Negative\",\"Positive\",\"Neutral\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17faa593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities function \n",
    "import re, unicodedata\n",
    "import pandas as pd\n",
    "\n",
    "URL_RE = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "TAG_RE = re.compile(r\"<[^>]+>\")\n",
    "MULTISPACE_RE = re.compile(r\"\\s+\")\n",
    "VIETNAMESE_BASIC_STOPWORDS = set(\"\"\"\n",
    "v√† ho·∫∑c nh∆∞ng l√† th√¨ m√† ƒë∆∞·ª£c b·ªã c·ªßa cho v·ªõi v·ªÅ t·ª´ t·ªõi ƒë·∫øn n·ªói do v√¨ n√™n n·∫øu khi ƒë·ªÉ b·∫±ng nh∆∞ l·∫°i ƒë√£ ƒëang s·∫Ω kh√¥ng ch∆∞a ch·∫≥ng r·∫•t qu√° l·∫Øm h∆°i\n",
    "n√†y kia n·ªç ƒë√≥ ƒë√¢y ·∫•y v·∫≠y th·∫ø sao t·∫°i v√¨ do ƒë√≥ tuy nhi√™n h∆°n k√©m ch·ªâ m·ªói m·ªôt c√°c nh·ªØng c√°i con chi·∫øc ƒë√¥i ƒëc nh√© nha ·∫° ∆°i\n",
    "\"\"\".split())\n",
    "\n",
    "def preprocess_xgb(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    text = text.strip().lower()\n",
    "    text = URL_RE.sub(\" \", text)\n",
    "    text = TAG_RE.sub(\" \", text)\n",
    "    text = text.replace(\"‚ù§Ô∏è\", \" yeu \").replace(\"‚ù§\", \" yeu \").replace(\"üòç\", \" yeu \")\n",
    "    text = re.sub(r\"[^\\w\\s√°√†·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√©√®·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√≠√¨·ªâƒ©·ªã√≥√≤·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√∫√π·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±√Ω·ª≥·ª∑·ªπ·ªµƒë]\", \" \", text)\n",
    "    text = unicodedata.normalize(\"NFC\", text)  # normalize accents\n",
    "    text = MULTISPACE_RE.sub(\" \", text).strip()\n",
    "    tokens = [w for w in text.split() if w not in VIETNAMESE_BASIC_STOPWORDS]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e29acf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n",
      "25/10/21 18:27:51 WARN Utils: Your hostname, DESKTOP-1JUULPQ resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/10/21 18:27:51 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/home/kaos/AfterGradEx/big_data/ABSA/.conda/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/kaos/.ivy2/cache\n",
      "The jars for the packages stored in: /home/kaos/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      "org.apache.kafka#kafka-clients added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e747c0b9-2e29-457d-9469-3d3170446e82;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.5.1 in central\n",
      "\tfound com.github.luben#zstd-jni;1.5.5-1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      ":: resolution report :: resolve 588ms :: artifacts dl 23ms\n",
      "\t:: modules in use:\n",
      "\tcom.github.luben#zstd-jni;1.5.5-1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.5.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 by [org.apache.kafka#kafka-clients;3.5.1] in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.1 by [org.xerial.snappy#snappy-java;1.1.10.3] in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.36 by [org.slf4j#slf4j-api;2.0.7] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   15  |   0   |   0   |   3   ||   12  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e747c0b9-2e29-457d-9469-3d3170446e82\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 12 already retrieved (0kB/14ms)\n",
      "25/10/21 18:27:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import findspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "try:\n",
    "    spark.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "scala_version = '2.12'\n",
    "spark_version = '3.5.1'\n",
    "packages = [ f'org.apache.spark:spark-sql-kafka-0-10_{scala_version}:{spark_version}' , 'org.apache.kafka:kafka-clients:3.5.1' ]\n",
    "\n",
    "findspark.init()\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"kafka\").config(\"spark.jars.packages\", \",\".join(packages)).getOrCreate()\n",
    "\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ba360d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_name = 'ABSA'\n",
    "kafka_server = 'localhost:9092'\n",
    "\n",
    "df_raw = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", kafka_server).option(\"subscribe\", topic_name).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2f06bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_raw .printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b21e58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing live view refreshed every 5 seconds\n",
      "Seconds passed: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>H√†ng ƒë·∫πp so v·ªõi gi√° nha .nh∆∞ng ƒë√≥ng g√≥i kh√¥ng ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D·ªãch v·ª• kh√°ch h√†ng gi·ªõi thi·ªáu s·∫£n ph·∫©m r·∫•t t·ªët...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>H√†ng ƒë·∫πp so v·ªõi gi√° nha .nh∆∞ng ƒë√≥ng g√≥i kh√¥ng ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D·ªãch v·ª• kh√°ch h√†ng gi·ªõi thi·ªáu s·∫£n ph·∫©m r·∫•t t·ªët...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>giao nhanh gi√†y ƒë·∫πp √™m ch√¢n nh∆∞ng from gi√†y h∆°...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ngo√†i ng·∫Øm c·ª±c ph·∫©m thi ƒë·∫•u trong s√¢n th√¨ ngo√†...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>D√†i xinh nha from l√™n ƒë·∫πp x·ªâu √™m ch√¢n l·∫Øm nha ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Giao h√†ng nhanh C√≥ Gi√†y kh√° c·ª©ng, l√≥t gi√†y th·ª´...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Gi·∫ßy ƒëi √™m, gi√° c·∫£ h·ª£p l√Ω, ƒë√≥ng g√≥i c·∫©n th·∫≠n, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>D√†y gi√° r·∫ª n√™n ƒë·ª´ng mong ƒë·ª£i j nhi·ªÅu, ti·ªÅn n√†o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>V·ªõi ph√≤ng b·ªánh kh√¥ng ƒë·∫∑c hi·ªáu : ƒê·ªÉ h·∫°n ch·∫ø l√¢y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>H√†ng r·∫•t ok</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ƒêi h∆°i c·ª©ng kh ƒë∆∞·ª£c √™m ch√¢n nh∆∞ng v·ªõi gi√° n√†y ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Review\n",
       "0   H√†ng ƒë·∫πp so v·ªõi gi√° nha .nh∆∞ng ƒë√≥ng g√≥i kh√¥ng ...\n",
       "1   D·ªãch v·ª• kh√°ch h√†ng gi·ªõi thi·ªáu s·∫£n ph·∫©m r·∫•t t·ªët...\n",
       "2   H√†ng ƒë·∫πp so v·ªõi gi√° nha .nh∆∞ng ƒë√≥ng g√≥i kh√¥ng ...\n",
       "3   D·ªãch v·ª• kh√°ch h√†ng gi·ªõi thi·ªáu s·∫£n ph·∫©m r·∫•t t·ªët...\n",
       "4   giao nhanh gi√†y ƒë·∫πp √™m ch√¢n nh∆∞ng from gi√†y h∆°...\n",
       "5   Ngo√†i ng·∫Øm c·ª±c ph·∫©m thi ƒë·∫•u trong s√¢n th√¨ ngo√†...\n",
       "6   D√†i xinh nha from l√™n ƒë·∫πp x·ªâu √™m ch√¢n l·∫Øm nha ...\n",
       "7   Giao h√†ng nhanh C√≥ Gi√†y kh√° c·ª©ng, l√≥t gi√†y th·ª´...\n",
       "8   Gi·∫ßy ƒëi √™m, gi√° c·∫£ h·ª£p l√Ω, ƒë√≥ng g√≥i c·∫©n th·∫≠n, ...\n",
       "9   D√†y gi√° r·∫ª n√™n ƒë·ª´ng mong ƒë·ª£i j nhi·ªÅu, ti·ªÅn n√†o...\n",
       "10  V·ªõi ph√≤ng b·ªánh kh√¥ng ƒë·∫∑c hi·ªáu : ƒê·ªÉ h·∫°n ch·∫ø l√¢y...\n",
       "11                                        H√†ng r·∫•t ok\n",
       "12  ƒêi h∆°i c·ª©ng kh ƒë∆∞·ª£c √™m ch√¢n nh∆∞ng v·ªõi gi√° n√†y ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "break\n",
      "Live view ended...\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "from IPython.display import display, clear_output\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "input_schema = StructType([\n",
    "    StructField(\"Review\", StringType()),\n",
    "])\n",
    "\n",
    "df_input = df_raw.selectExpr(\"CAST(value AS STRING) as json\") \\\n",
    "    .select(from_json(col(\"json\"), input_schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "\n",
    "for i in range(0, 100):\n",
    "    try:\n",
    "        print(\"Showing live view refreshed every 5 seconds\")\n",
    "        print(f\"Seconds passed: {i*5}\")\n",
    "        display(df_input.toPandas())\n",
    "        sleep(5)\n",
    "        clear_output(wait=True)\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"break\")\n",
    "        break\n",
    "\n",
    "print(\"Live view ended...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a29beba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import SparkSession\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# Pre-load models/encoders and broadcast them\n",
    "bc_models = {\n",
    "    aspect: spark.sparkContext.broadcast(joblib.load(os.path.join(MODEL_DIR, f\"{aspect}_xgb.pkl\")))\n",
    "    for aspect in ASPECTS\n",
    "}\n",
    "bc_encoders = {\n",
    "    aspect: spark.sparkContext.broadcast(joblib.load(os.path.join(MODEL_DIR, f\"{aspect}_encoder.pkl\")))\n",
    "    for aspect in ASPECTS\n",
    "}\n",
    "\n",
    "\n",
    "# ----------------------\n",
    "# UDF factory for each aspect\n",
    "# ----------------------\n",
    "def make_predict_udf(aspect):\n",
    "    model = bc_models[aspect].value\n",
    "    encoder = bc_encoders[aspect].value\n",
    "\n",
    "    @pandas_udf(StringType())\n",
    "    def predict_udf(texts: pd.Series) -> pd.Series:\n",
    "        texts_proc = [preprocess_xgb(t) for t in texts]\n",
    "        preds_encoded = model.predict(texts_proc)\n",
    "        preds_original = encoder.inverse_transform(preds_encoded)\n",
    "        return pd.Series([SENT_ID2NAME[int(x)] for x in preds_original])\n",
    "\n",
    "    return predict_udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca285fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Batch 0 ===\n",
      "+----+------+----------+-------------+------------+------------+---------+-----------------+------------+-----------+\n",
      "|MSSV|Review|Price_pred|Shipping_pred|Outlook_pred|Quality_pred|Size_pred|Shop_Service_pred|General_pred|Others_pred|\n",
      "+----+------+----------+-------------+------------+------------+---------+-----------------+------------+-----------+\n",
      "+----+------+----------+-------------+------------+------------+---------+-----------------+------------+-----------+\n",
      "\n",
      "\n",
      "=== Batch 1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+---------------------------------+----------+-------------+------------+------------+---------+-----------------+------------+-----------+\n",
      "|MSSV                     |Review                           |Price_pred|Shipping_pred|Outlook_pred|Quality_pred|Size_pred|Shop_Service_pred|General_pred|Others_pred|\n",
      "+-------------------------+---------------------------------+----------+-------------+------------+------------+---------+-----------------+------------+-----------+\n",
      "|NguyenThanhDanh_240201008|gi√†y ƒë·∫πp gi√° r·∫ª m·ªçi ng∆∞·ªùi n√™n mua|Positive  |None         |Positive    |None        |None     |None             |None        |None       |\n",
      "+-------------------------+---------------------------------+----------+-------------+------------+------------+---------+-----------------+------------+-----------+\n",
      "\n",
      "\n",
      "=== Batch 2 ===\n",
      "+-------------------------+----------------------------------------------------------------------------------------------------------+----------+-------------+------------+------------+---------+-----------------+------------+-----------+\n",
      "|MSSV                     |Review                                                                                                    |Price_pred|Shipping_pred|Outlook_pred|Quality_pred|Size_pred|Shop_Service_pred|General_pred|Others_pred|\n",
      "+-------------------------+----------------------------------------------------------------------------------------------------------+----------+-------------+------------+------------+---------+-----------------+------------+-----------+\n",
      "|NguyenThanhDanh_240201008|gi√†y ƒë·∫πp ƒëi cao qu√¢ tr·ªùi, ƒë√≥ng g√≥i c·∫©n th·∫≠n giao nhanh, ch·ªâ m·ªói l√† h∆°i c√≥ m·∫•y v·ªát ƒëen nh·ªè th√¥i c√≤n l·∫°i oke|None      |Positive     |Positive    |None        |None     |Positive         |None        |None       |\n",
      "+-------------------------+----------------------------------------------------------------------------------------------------------+----------+-------------+------------+------------+---------+-----------------+------------+-----------+\n",
      "\n",
      "\n",
      "=== Batch 3 ===\n",
      "+-------------------------+------------------------------------+----------+-------------+------------+------------+---------+-----------------+------------+-----------+\n",
      "|MSSV                     |Review                              |Price_pred|Shipping_pred|Outlook_pred|Quality_pred|Size_pred|Shop_Service_pred|General_pred|Others_pred|\n",
      "+-------------------------+------------------------------------+----------+-------------+------------+------------+---------+-----------------+------------+-----------+\n",
      "|NguyenThanhDanh_240201008|Tuy·ªát v·ªùi ch√∫c shop bu√¥n may b√°n ƒë·∫Øt|None      |None         |None        |None        |None     |None             |Positive    |None       |\n",
      "+-------------------------+------------------------------------+----------+-------------+------------+------------+---------+-----------------+------------+-----------+\n",
      "\n",
      "\n",
      "=== Batch 4 ===\n",
      "+-------------------------+-------------------------------------------------------+----------+-------------+------------+------------+---------+-----------------+------------+-----------+\n",
      "|MSSV                     |Review                                                 |Price_pred|Shipping_pred|Outlook_pred|Quality_pred|Size_pred|Shop_Service_pred|General_pred|Others_pred|\n",
      "+-------------------------+-------------------------------------------------------+----------+-------------+------------+------------+---------+-----------------+------------+-----------+\n",
      "|NguyenThanhDanh_240201008|ch·∫•t li·ªáu ƒë·∫πp h∆°n so v·ªõi gi√° ti·ªÅn m·ªçi ng∆∞·ªùi l√™n mua nh√©|None      |None         |None        |Positive    |None     |None             |None        |None       |\n",
      "+-------------------------+-------------------------------------------------------+----------+-------------+------------+------------+---------+-----------------+------------+-----------+\n",
      "\n",
      "\n",
      "=== Batch 5 ===\n",
      "+-------------------------+--------------------------------------------------------+----------+-------------+------------+------------+---------+-----------------+------------+-----------+\n",
      "|MSSV                     |Review                                                  |Price_pred|Shipping_pred|Outlook_pred|Quality_pred|Size_pred|Shop_Service_pred|General_pred|Others_pred|\n",
      "+-------------------------+--------------------------------------------------------+----------+-------------+------------+------------+---------+-----------------+------------+-----------+\n",
      "|NguyenThanhDanh_240201008|Giao h√†ng nhanh, ƒë√∫ng size, from chu·∫©n √™m s·∫Ω ·ªßng h·ªô th√™m|None      |Positive     |Positive    |Positive    |Positive |None             |None        |None       |\n",
      "+-------------------------+--------------------------------------------------------+----------+-------------+------------+------------+---------+-----------------+------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/kaos/AfterGradEx/big_data/ABSA/.conda/lib/python3.11/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kaos/AfterGradEx/big_data/ABSA/.conda/lib/python3.11/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/kaos/AfterGradEx/big_data/ABSA/.conda/lib/python3.11/socket.py\", line 718, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# WriteStream triggers every 5 seconds\u001b[39;00m\n\u001b[32m     30\u001b[39m query = (\n\u001b[32m     31\u001b[39m     df_input.writeStream\n\u001b[32m     32\u001b[39m     .foreachBatch(foreach_batch)\n\u001b[32m     33\u001b[39m     .trigger(processingTime=\u001b[33m\"\u001b[39m\u001b[33m5 seconds\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     34\u001b[39m     .start()\n\u001b[32m     35\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[43mquery\u001b[49m\u001b[43m.\u001b[49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AfterGradEx/big_data/ABSA/.conda/lib/python3.11/site-packages/pyspark/sql/streaming/query.py:221\u001b[39m, in \u001b[36mStreamingQuery.awaitTermination\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    219\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jsq.awaitTermination(\u001b[38;5;28mint\u001b[39m(timeout * \u001b[32m1000\u001b[39m))\n\u001b[32m    220\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jsq\u001b[49m\u001b[43m.\u001b[49m\u001b[43mawaitTermination\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AfterGradEx/big_data/ABSA/.conda/lib/python3.11/site-packages/py4j/java_gateway.py:1321\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1314\u001b[39m args_command, temp_args = \u001b[38;5;28mself\u001b[39m._build_args(*args)\n\u001b[32m   1316\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1317\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1318\u001b[39m     args_command +\\\n\u001b[32m   1319\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m-> \u001b[39m\u001b[32m1321\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1322\u001b[39m return_value = get_return_value(\n\u001b[32m   1323\u001b[39m     answer, \u001b[38;5;28mself\u001b[39m.gateway_client, \u001b[38;5;28mself\u001b[39m.target_id, \u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m   1325\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AfterGradEx/big_data/ABSA/.conda/lib/python3.11/site-packages/py4j/java_gateway.py:1038\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1036\u001b[39m connection = \u001b[38;5;28mself\u001b[39m._get_connection()\n\u001b[32m   1037\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[32m   1040\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m._create_connection_guard(connection)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AfterGradEx/big_data/ABSA/.conda/lib/python3.11/site-packages/py4j/clientserver.py:511\u001b[39m, in \u001b[36mClientServerConnection.send_command\u001b[39m\u001b[34m(self, command)\u001b[39m\n\u001b[32m    509\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    510\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m         answer = smart_decode(\u001b[38;5;28mself\u001b[39m.stream.readline()[:-\u001b[32m1\u001b[39m])\n\u001b[32m    512\u001b[39m         logger.debug(\u001b[33m\"\u001b[39m\u001b[33mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m\"\u001b[39m.format(answer))\n\u001b[32m    513\u001b[39m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[32m    514\u001b[39m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/AfterGradEx/big_data/ABSA/.conda/lib/python3.11/socket.py:718\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    716\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m718\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    720\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_json, col, lit\n",
    "from pyspark.sql.types import StructType, StringType, StructField\n",
    "\n",
    "input_schema = StructType([\n",
    "    StructField(\"Review\", StringType()),\n",
    "])\n",
    "\n",
    "df_input = (\n",
    "    df_raw\n",
    "    .selectExpr(\"CAST(value AS STRING) as json\")\n",
    "    .select(from_json(col(\"json\"), input_schema).alias(\"data\"))\n",
    "    .select(\"data.*\")\n",
    ")\n",
    "\n",
    "df_input = df_input.withColumn(\"MSSV\", lit(\"NguyenThanhDanh_240201008\"))\n",
    "\n",
    "# Apply UDFs\n",
    "for aspect in ASPECTS:\n",
    "    predict_udf = make_predict_udf(aspect)\n",
    "    df_input = df_input.withColumn(f\"{aspect}_pred\", predict_udf(\"Review\"))\n",
    "\n",
    "cols_to_show = [\"MSSV\", \"Review\"] + [f\"{aspect}_pred\" for aspect in ASPECTS]\n",
    "\n",
    "# Define per-batch function for live console output\n",
    "def foreach_batch(df, batch_id):\n",
    "    print(f\"\\n=== Batch {batch_id} ===\")\n",
    "    df.select(*cols_to_show).show(10, truncate=False)\n",
    "\n",
    "# WriteStream triggers every 5 seconds\n",
    "query = (\n",
    "    df_input.writeStream\n",
    "    .foreachBatch(foreach_batch)\n",
    "    .trigger(processingTime=\"5 seconds\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ab8bb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping numpy as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Collecting numpy==1.26.4\n",
      "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-1.26.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip uninstall numpy -y\n",
    "%pip install numpy==1.26.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f332e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark==3.5.1 in ./.conda/lib/python3.11/site-packages (3.5.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in ./.conda/lib/python3.11/site-packages (from pyspark==3.5.1) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in ./.conda/lib/python3.11/site-packages (2.3.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in ./.conda/lib/python3.11/site-packages (from pandas) (2.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.conda/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.conda/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.conda/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: findspark in ./.conda/lib/python3.11/site-packages (2.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: joblib in ./.conda/lib/python3.11/site-packages (1.5.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in ./.conda/lib/python3.11/site-packages (1.7.2)\n",
      "Requirement already satisfied: numpy>=1.22.0 in ./.conda/lib/python3.11/site-packages (from scikit-learn) (2.3.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in ./.conda/lib/python3.11/site-packages (from scikit-learn) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.conda/lib/python3.11/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.conda/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pyvi in ./.conda/lib/python3.11/site-packages (0.1.1)\n",
      "Requirement already satisfied: scikit-learn in ./.conda/lib/python3.11/site-packages (from pyvi) (1.7.2)\n",
      "Requirement already satisfied: sklearn-crfsuite in ./.conda/lib/python3.11/site-packages (from pyvi) (0.5.0)\n",
      "Requirement already satisfied: numpy>=1.22.0 in ./.conda/lib/python3.11/site-packages (from scikit-learn->pyvi) (2.3.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in ./.conda/lib/python3.11/site-packages (from scikit-learn->pyvi) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.conda/lib/python3.11/site-packages (from scikit-learn->pyvi) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.conda/lib/python3.11/site-packages (from scikit-learn->pyvi) (3.6.0)\n",
      "Requirement already satisfied: python-crfsuite>=0.9.7 in ./.conda/lib/python3.11/site-packages (from sklearn-crfsuite->pyvi) (0.9.11)\n",
      "Requirement already satisfied: tabulate>=0.4.2 in ./.conda/lib/python3.11/site-packages (from sklearn-crfsuite->pyvi) (0.9.0)\n",
      "Requirement already satisfied: tqdm>=2.0 in ./.conda/lib/python3.11/site-packages (from sklearn-crfsuite->pyvi) (4.67.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pyarrow in ./.conda/lib/python3.11/site-packages (21.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting numpy==1.26.4\n",
      "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.3.3\n",
      "    Uninstalling numpy-2.3.3:\n",
      "      Successfully uninstalled numpy-2.3.3\n",
      "Successfully installed numpy-1.26.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark==3.5.1\n",
    "%pip install pandas\n",
    "%pip install findspark\n",
    "%pip install joblib\n",
    "%pip install scikit-learn\n",
    "%pip install pyvi\n",
    "%pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7625c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# PATH definitions\n",
    "DATA_DIR = \"./dataset\"  # Ä‘á»•i náº¿u dÃ¹ng Drive\n",
    "MODEL_DIR = \"./torch_models\"\n",
    "TRAIN_PATH = os.path.join(DATA_DIR, \"train_data.csv\")\n",
    "VAL_PATH   = os.path.join(DATA_DIR, \"val_data.csv\")\n",
    "TEST_PATH  = os.path.join(DATA_DIR, \"test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16759867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns\n",
    "TEXT_COL = \"Review\"\n",
    "ASPECTS  = [\"Price\",\"Shipping\",\"Outlook\",\"Quality\",\"Size\",\"Shop_Service\",\"General\",\"Others\"]\n",
    "\n",
    "# Label mapping\n",
    "SENT_ID2NAME = {-1: \"None\", 0: \"Negative\", 1: \"Positive\", 2: \"Neutral\"}\n",
    "LABEL_VALUES = list(SENT_ID2NAME.keys())  # [-1, 0, 1, 2]\n",
    "LABEL_NAMES  = list(SENT_ID2NAME.values())  # [\"None\",\"Negative\",\"Positive\",\"Neutral\"]\n",
    "\n",
    "SEQ_LEN = 50        # max number of words per review\n",
    "EMBEDDING_DIM = 300 # embedding vector size\n",
    "MAX_WORDS = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17faa593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities function \n",
    "import re, unicodedata\n",
    "import pandas as pd\n",
    "from pyvi import ViTokenizer\n",
    "\n",
    "URL_RE = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "TAG_RE = re.compile(r\"<[^>]+>\")\n",
    "MULTISPACE_RE = re.compile(r\"\\s+\")\n",
    "VIETNAMESE_BASIC_STOPWORDS = set(\"\"\"\n",
    "vÃ  hoáº·c nhÆ°ng lÃ  thÃ¬ mÃ  Ä‘Æ°á»£c bá»‹ cá»§a cho vá»›i vá» tá»« tá»›i Ä‘áº¿n ná»—i do vÃ¬ nÃªn náº¿u khi Ä‘á»ƒ báº±ng nhÆ° láº¡i Ä‘Ã£ Ä‘ang sáº½ khÃ´ng chÆ°a cháº³ng ráº¥t quÃ¡ láº¯m hÆ¡i\n",
    "nÃ y kia ná» Ä‘Ã³ Ä‘Ã¢y áº¥y váº­y tháº¿ sao táº¡i vÃ¬ do Ä‘Ã³ tuy nhiÃªn hÆ¡n kÃ©m chá»‰ má»—i má»™t cÃ¡c nhá»¯ng cÃ¡i con chiáº¿c Ä‘Ã´i Ä‘c nhÃ© nha áº¡ Æ¡i\n",
    "\"\"\".split())\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    text = text.strip().lower()\n",
    "    text = re.sub(r\"https?://\\S+|www\\.\\S+\", \" \", text)\n",
    "    text = re.sub(r\"<[^>]+>\", \" \", text)\n",
    "    text = text.replace(\"â¤ï¸\", \" yeu \").replace(\"â¤\", \" yeu \").replace(\"ğŸ˜\", \" yeu \")\n",
    "    text = re.sub(r\"[^\\w\\sÃ¡Ã áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã©Ã¨áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã­Ã¬á»‰Ä©á»‹Ã³Ã²á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£ÃºÃ¹á»§Å©á»¥Æ°á»©á»«á»­á»¯á»±Ã½á»³á»·á»¹á»µÄ‘]\", \" \", text)\n",
    "    text = unicodedata.normalize(\"NFC\", text)\n",
    "    text = MULTISPACE_RE.sub(\" \", text)\n",
    "    tokens = [w for w in ViTokenizer.tokenize(text).split() if w not in VIETNAMESE_BASIC_STOPWORDS]\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0248158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_merge(train_path, val_path, test_path):\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    val_df   = pd.read_csv(val_path)\n",
    "    test_df  = pd.read_csv(test_path)\n",
    "\n",
    "    # Merge train + val\n",
    "    full_train = pd.concat([train_df, val_df], axis=0).reset_index(drop=True)\n",
    "\n",
    "    # Preprocess review text\n",
    "    full_train[TEXT_COL] = full_train[TEXT_COL].map(preprocess_text)\n",
    "    test_df[TEXT_COL]    = test_df[TEXT_COL].map(preprocess_text)\n",
    "\n",
    "    return full_train, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "677a5027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torch-2.5.1%2Bcu121-cp311-cp311-linux_x86_64.whl (780.5 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m780.5/780.5 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m  \u001b[33m0:00:18\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchtext\n",
      "  Downloading https://download.pytorch.org/whl/torchtext-0.17.0%2Bcpu-cp311-cp311-linux_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting filelock (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.conda/lib/python3.11/site-packages (from torch) (4.15.0)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/Jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Collecting triton==3.1.0 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting sympy==1.13.1 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m57.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
      "  Downloading https://download.pytorch.org/whl/nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Using cached https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Requirement already satisfied: tqdm in ./.conda/lib/python3.11/site-packages (from torchtext) (4.67.1)\n",
      "Requirement already satisfied: requests in ./.conda/lib/python3.11/site-packages (from torchtext) (2.32.5)\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu121/torch-2.2.0%2Bcu121-cp311-cp311-linux_x86_64.whl (757.3 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m757.3/757.3 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m  \u001b[33m0:00:20\u001b[0m:00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in ./.conda/lib/python3.11/site-packages (from torchtext) (2.3.3)\n",
      "Collecting torchdata==0.7.1 (from torchtext)\n",
      "  Downloading https://download.pytorch.org/whl/torchdata-0.7.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sympy (from torch)\n",
      "  Using cached https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m  \u001b[33m0:00:16\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/cu121/nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting triton==2.2.0 (from torch)\n",
      "  Downloading https://download.pytorch.org/whl/triton-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m  \u001b[33m0:00:07\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: urllib3>=1.25 in ./.conda/lib/python3.11/site-packages (from torchdata==0.7.1->torchtext) (2.5.0)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Downloading https://download.pytorch.org/whl/MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.conda/lib/python3.11/site-packages (from requests->torchtext) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.conda/lib/python3.11/site-packages (from requests->torchtext) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.conda/lib/python3.11/site-packages (from requests->torchtext) (2025.8.3)\n",
      "Using cached https://download.pytorch.org/whl/filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Using cached https://download.pytorch.org/whl/fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Downloading https://download.pytorch.org/whl/Jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Downloading https://download.pytorch.org/whl/networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading https://download.pytorch.org/whl/nvidia_nvjitlink_cu12-12.9.86-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.7/39.7 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: mpmath, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, MarkupSafe, fsspec, filelock, triton, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, torchdata, torchtext\n",
      "\u001b[2K  Attempting uninstall: nvidia-nccl-cu12â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 3/23\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K    Found existing installation: nvidia-nccl-cu12 2.28.3â”â”â”â”â”â”\u001b[0m \u001b[32m 3/23\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K    Uninstalling nvidia-nccl-cu12-2.28.3:â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 3/23\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K      Successfully uninstalled nvidia-nccl-cu12-2.28.3â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m 3/23\u001b[0m [nvidia-nvjitlink-cu12]\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23/23\u001b[0m [torchtext]23\u001b[0m [torchtext]olver-cu12]2]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-2.1.5 filelock-3.13.1 fsspec-2024.6.1 jinja2-3.1.4 mpmath-1.3.0 networkx-3.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.9.86 nvidia-nvtx-cu12-12.1.105 sympy-1.13.1 torch-2.2.0+cu121 torchdata-0.7.1 torchtext-0.17.0+cpu triton-2.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchtext --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d09b404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "SEQ_LEN = 50\n",
    "MAX_WORDS = 20000\n",
    "\n",
    "def yield_tokens(texts):\n",
    "    for text in texts:\n",
    "        yield text.split()\n",
    "\n",
    "def build_vocab(texts):\n",
    "    vocab = build_vocab_from_iterator(yield_tokens(texts), specials=[\"<unk>\"])\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "    return vocab\n",
    "\n",
    "def texts_to_sequences(texts, vocab, seq_len=SEQ_LEN):\n",
    "    sequences = []\n",
    "    for text in texts:\n",
    "        tokens = text.split()\n",
    "        seq = [vocab[token] for token in tokens]\n",
    "        # pad or truncate\n",
    "        if len(seq) < seq_len:\n",
    "            seq = seq + [0]*(seq_len - len(seq))\n",
    "        else:\n",
    "            seq = seq[:seq_len]\n",
    "        sequences.append(seq)\n",
    "    return torch.tensor(sequences, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db90e5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNTextClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, seq_len, num_classes, embedding_weights=None):\n",
    "        super().__init__()\n",
    "        if embedding_weights is not None:\n",
    "            self.embedding = nn.Embedding.from_pretrained(embedding_weights, freeze=False)\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.conv = nn.Conv1d(in_channels=embed_dim, out_channels=128, kernel_size=3)\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.fc = nn.Linear(128, 64)\n",
    "        self.out = nn.Linear(64, num_classes)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)           # [B, seq_len, embed_dim]\n",
    "        x = x.permute(0, 2, 1)         # [B, embed_dim, seq_len]\n",
    "        x = F.relu(self.conv(x))       # [B, 128, L]\n",
    "        x = self.pool(x).squeeze(-1)   # [B, 128]\n",
    "        x = F.relu(self.fc(x))         # [B, 64]\n",
    "        x = self.dropout(x)\n",
    "        return self.out(x)             # [B, num_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f762269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def train_cnn_models(train_df, test_df, vocab, seq_len=50, embed_dim=300, batch_size=32, epochs=5):\n",
    "    \"\"\"\n",
    "    Train CNN models for multi-aspect classification like XGB function structure.\n",
    "    Returns:\n",
    "        models: dict of trained CNN models per aspect\n",
    "        encoders: dict of LabelEncoders per aspect\n",
    "        reports: dict of sklearn classification reports per aspect\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    encoders = {}\n",
    "    reports = {}\n",
    "\n",
    "    # Convert train/test text to sequences once\n",
    "    X_train_seq = texts_to_sequences(train_df[TEXT_COL], vocab, seq_len)\n",
    "    X_test_seq  = texts_to_sequences(test_df[TEXT_COL], vocab, seq_len)\n",
    "\n",
    "    for aspect in ASPECTS:\n",
    "        print(f\"\\n=== Training aspect: {aspect} ===\")\n",
    "\n",
    "        # Encode labels\n",
    "        le = LabelEncoder()\n",
    "        y_train = torch.tensor(le.fit_transform(train_df[aspect]), dtype=torch.long)\n",
    "        y_test  = torch.tensor(le.transform(test_df[aspect]), dtype=torch.long)\n",
    "        encoders[aspect] = le\n",
    "\n",
    "        # Build dataset & loader\n",
    "        train_dataset = TensorDataset(X_train_seq, y_train)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        # Build CNN model\n",
    "        model = CNNTextClassifier(vocab_size=len(vocab), embed_dim=embed_dim, seq_len=seq_len, num_classes=len(le.classes_))\n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Train\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            total_loss = 0\n",
    "            for xb, yb in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                logits = model(xb)\n",
    "                loss = criterion(logits, yb)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            print(f\"Epoch {epoch+1}/{epochs} - Loss: {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits_test = model(X_test_seq)\n",
    "            preds_test = torch.argmax(logits_test, dim=1).numpy()\n",
    "\n",
    "        report = classification_report(\n",
    "            le.inverse_transform(y_test),\n",
    "            le.inverse_transform(preds_test),\n",
    "            labels=LABEL_VALUES,\n",
    "            target_names=[SENT_ID2NAME[i] for i in LABEL_VALUES]\n",
    "        )\n",
    "        print(report)\n",
    "\n",
    "        models[aspect] = model\n",
    "        reports[aspect] = report\n",
    "\n",
    "    return models, encoders, reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8ad9c610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_aspects_cnn(models, encoders, texts, vocab, seq_len=50):\n",
    "    \"\"\"\n",
    "    Predict multiple aspects for a list of texts using trained CNN models.\n",
    "    Returns a pandas DataFrame with each aspect's prediction.\n",
    "    \"\"\"\n",
    "    # Preprocess texts and convert to sequences\n",
    "    sequences = texts_to_sequences([preprocess_text(t) for t in texts], vocab, seq_len)\n",
    "    \n",
    "    results = {}\n",
    "    for aspect, model in models.items():\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = model(sequences)\n",
    "            pred_encoded = torch.argmax(logits, dim=1).numpy()\n",
    "        # Map back to original label\n",
    "        pred_original = encoders[aspect].inverse_transform(pred_encoded)\n",
    "        # Map to SENT_ID2NAME for display\n",
    "        results[aspect] = [SENT_ID2NAME[int(x)] for x in pred_original]\n",
    "    \n",
    "    return pd.DataFrame(results, index=range(len(texts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4b592a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "train_df, test_df = load_and_merge(TRAIN_PATH, VAL_PATH, TEST_PATH)\n",
    "vocab = build_vocab(train_df[TEXT_COL])\n",
    "X_train_seq = texts_to_sequences(train_df[TEXT_COL], vocab)\n",
    "X_test_seq  = texts_to_sequences(test_df[TEXT_COL], vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef49103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training aspect: Price ===\n",
      "Epoch 1/10 - Loss: 0.2821\n",
      "Epoch 2/10 - Loss: 0.1679\n",
      "Epoch 3/10 - Loss: 0.1234\n",
      "Epoch 4/10 - Loss: 0.0819\n",
      "Epoch 5/10 - Loss: 0.0505\n",
      "Epoch 6/10 - Loss: 0.0355\n",
      "Epoch 7/10 - Loss: 0.0309\n",
      "Epoch 8/10 - Loss: 0.0211\n",
      "Epoch 9/10 - Loss: 0.0228\n",
      "Epoch 10/10 - Loss: 0.0147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaos/AfterGradEx/big_data/ABSA/.conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/kaos/AfterGradEx/big_data/ABSA/.conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/kaos/AfterGradEx/big_data/ABSA/.conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        None       0.96      0.96      0.96      1999\n",
      "    Negative       0.00      0.00      0.00         3\n",
      "    Positive       0.76      0.83      0.79       247\n",
      "     Neutral       0.47      0.37      0.42        91\n",
      "\n",
      "    accuracy                           0.92      2340\n",
      "   macro avg       0.55      0.54      0.54      2340\n",
      "weighted avg       0.92      0.92      0.92      2340\n",
      "\n",
      "\n",
      "=== Training aspect: Shipping ===\n",
      "Epoch 1/10 - Loss: 0.2974\n",
      "Epoch 2/10 - Loss: 0.1627\n",
      "Epoch 3/10 - Loss: 0.1151\n",
      "Epoch 4/10 - Loss: 0.0761\n",
      "Epoch 5/10 - Loss: 0.0499\n",
      "Epoch 6/10 - Loss: 0.0345\n",
      "Epoch 7/10 - Loss: 0.0247\n",
      "Epoch 8/10 - Loss: 0.0252\n",
      "Epoch 9/10 - Loss: 0.0183\n",
      "Epoch 10/10 - Loss: 0.0202\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        None       0.96      0.98      0.97      1635\n",
      "    Negative       0.85      0.75      0.80       124\n",
      "    Positive       0.92      0.94      0.93       549\n",
      "     Neutral       0.12      0.03      0.05        32\n",
      "\n",
      "    accuracy                           0.94      2340\n",
      "   macro avg       0.72      0.67      0.69      2340\n",
      "weighted avg       0.94      0.94      0.94      2340\n",
      "\n",
      "\n",
      "=== Training aspect: Outlook ===\n",
      "Epoch 1/10 - Loss: 0.5506\n",
      "Epoch 2/10 - Loss: 0.3690\n",
      "Epoch 3/10 - Loss: 0.2615\n",
      "Epoch 4/10 - Loss: 0.1869\n",
      "Epoch 5/10 - Loss: 0.1282\n",
      "Epoch 6/10 - Loss: 0.0861\n",
      "Epoch 7/10 - Loss: 0.0672\n",
      "Epoch 8/10 - Loss: 0.0553\n",
      "Epoch 9/10 - Loss: 0.0374\n",
      "Epoch 10/10 - Loss: 0.0338\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        None       0.86      0.91      0.88      1069\n",
      "    Negative       0.57      0.37      0.45        95\n",
      "    Positive       0.90      0.84      0.87      1118\n",
      "     Neutral       0.15      0.22      0.18        58\n",
      "\n",
      "    accuracy                           0.84      2340\n",
      "   macro avg       0.62      0.59      0.60      2340\n",
      "weighted avg       0.85      0.84      0.84      2340\n",
      "\n",
      "\n",
      "=== Training aspect: Quality ===\n",
      "Epoch 1/10 - Loss: 0.5950\n",
      "Epoch 2/10 - Loss: 0.4028\n",
      "Epoch 3/10 - Loss: 0.2906\n",
      "Epoch 4/10 - Loss: 0.2001\n",
      "Epoch 5/10 - Loss: 0.1431\n",
      "Epoch 6/10 - Loss: 0.1002\n",
      "Epoch 7/10 - Loss: 0.0677\n",
      "Epoch 8/10 - Loss: 0.0482\n",
      "Epoch 9/10 - Loss: 0.0499\n",
      "Epoch 10/10 - Loss: 0.0536\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        None       0.89      0.93      0.91      1654\n",
      "    Negative       0.44      0.38      0.40        98\n",
      "    Positive       0.79      0.69      0.74       478\n",
      "     Neutral       0.30      0.27      0.29       110\n",
      "\n",
      "    accuracy                           0.83      2340\n",
      "   macro avg       0.60      0.57      0.58      2340\n",
      "weighted avg       0.82      0.83      0.82      2340\n",
      "\n",
      "\n",
      "=== Training aspect: Size ===\n",
      "Epoch 1/10 - Loss: 0.3761\n",
      "Epoch 2/10 - Loss: 0.2374\n",
      "Epoch 3/10 - Loss: 0.1818\n",
      "Epoch 4/10 - Loss: 0.1394\n",
      "Epoch 5/10 - Loss: 0.1020\n",
      "Epoch 6/10 - Loss: 0.0764\n",
      "Epoch 7/10 - Loss: 0.0535\n",
      "Epoch 8/10 - Loss: 0.0370\n",
      "Epoch 9/10 - Loss: 0.0319\n",
      "Epoch 10/10 - Loss: 0.0291\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        None       0.95      0.97      0.96      1953\n",
      "    Negative       0.60      0.44      0.51       125\n",
      "    Positive       0.71      0.73      0.72       165\n",
      "     Neutral       0.42      0.40      0.41        97\n",
      "\n",
      "    accuracy                           0.90      2340\n",
      "   macro avg       0.67      0.63      0.65      2340\n",
      "weighted avg       0.89      0.90      0.89      2340\n",
      "\n",
      "\n",
      "=== Training aspect: Shop_Service ===\n",
      "Epoch 1/10 - Loss: 0.4818\n",
      "Epoch 2/10 - Loss: 0.3114\n",
      "Epoch 3/10 - Loss: 0.2193\n",
      "Epoch 4/10 - Loss: 0.1536\n",
      "Epoch 5/10 - Loss: 0.1103\n",
      "Epoch 6/10 - Loss: 0.0734\n",
      "Epoch 7/10 - Loss: 0.0567\n",
      "Epoch 8/10 - Loss: 0.0416\n",
      "Epoch 9/10 - Loss: 0.0332\n",
      "Epoch 10/10 - Loss: 0.0407\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        None       0.94      0.93      0.94      1740\n",
      "    Negative       0.70      0.44      0.54       140\n",
      "    Positive       0.70      0.85      0.77       431\n",
      "     Neutral       0.00      0.00      0.00        29\n",
      "\n",
      "    accuracy                           0.88      2340\n",
      "   macro avg       0.58      0.56      0.56      2340\n",
      "weighted avg       0.87      0.88      0.87      2340\n",
      "\n",
      "\n",
      "=== Training aspect: General ===\n",
      "Epoch 1/10 - Loss: 0.5554\n",
      "Epoch 2/10 - Loss: 0.4205\n",
      "Epoch 3/10 - Loss: 0.3249\n",
      "Epoch 4/10 - Loss: 0.2294\n",
      "Epoch 5/10 - Loss: 0.1487\n",
      "Epoch 6/10 - Loss: 0.0990\n",
      "Epoch 7/10 - Loss: 0.0869\n",
      "Epoch 8/10 - Loss: 0.0627\n",
      "Epoch 9/10 - Loss: 0.0522\n",
      "Epoch 10/10 - Loss: 0.0451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaos/AfterGradEx/big_data/ABSA/.conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/kaos/AfterGradEx/big_data/ABSA/.conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/kaos/AfterGradEx/big_data/ABSA/.conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        None       0.90      0.89      0.90      1861\n",
      "    Negative       0.00      0.00      0.00        11\n",
      "    Positive       0.51      0.55      0.53       285\n",
      "     Neutral       0.46      0.46      0.46       183\n",
      "\n",
      "    accuracy                           0.81      2340\n",
      "   macro avg       0.47      0.48      0.47      2340\n",
      "weighted avg       0.81      0.81      0.81      2340\n",
      "\n",
      "\n",
      "=== Training aspect: Others ===\n",
      "Epoch 1/10 - Loss: 0.1610\n",
      "Epoch 2/10 - Loss: 0.0762\n",
      "Epoch 3/10 - Loss: 0.0326\n",
      "Epoch 4/10 - Loss: 0.0193\n",
      "Epoch 5/10 - Loss: 0.0133\n",
      "Epoch 6/10 - Loss: 0.0146\n",
      "Epoch 7/10 - Loss: 0.0124\n",
      "Epoch 8/10 - Loss: 0.0053\n",
      "Epoch 9/10 - Loss: 0.0071\n",
      "Epoch 10/10 - Loss: 0.0041\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        None       0.97      0.98      0.98      2151\n",
      "    Negative       0.00      0.00      0.00         0\n",
      "    Positive       0.00      0.00      0.00         0\n",
      "     Neutral       0.78      0.65      0.71       189\n",
      "\n",
      "    accuracy                           0.96      2340\n",
      "   macro avg       0.44      0.41      0.42      2340\n",
      "weighted avg       0.95      0.96      0.95      2340\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kaos/AfterGradEx/big_data/ABSA/.conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/kaos/AfterGradEx/big_data/ABSA/.conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/kaos/AfterGradEx/big_data/ABSA/.conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/kaos/AfterGradEx/big_data/ABSA/.conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/kaos/AfterGradEx/big_data/ABSA/.conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/kaos/AfterGradEx/big_data/ABSA/.conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/kaos/AfterGradEx/big_data/ABSA/.conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/kaos/AfterGradEx/big_data/ABSA/.conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/kaos/AfterGradEx/big_data/ABSA/.conda/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cnn_models, cnn_encoders, cnn_reports = train_cnn_models(\n",
    "    train_df,\n",
    "    test_df,\n",
    "    vocab=vocab,\n",
    "    seq_len=SEQ_LEN,\n",
    "    embed_dim=EMBEDDING_DIM,\n",
    "    batch_size=32,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0afb8be3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CNN Predictions ===\n",
      "     Price  Shipping   Outlook Quality  Size Shop_Service General Others\n",
      "0  Neutral  Negative  Positive    None  None         None    None   None\n",
      "1     None  Negative  Negative    None  None         None    None   None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "sample_texts = [\n",
    "    \"GiÃ y ráº¥t Ä‘áº¹p, giÃ¡ há»£p lÃ½ nhÆ°ng giao hÃ ng cháº­m\",\n",
    "    \"GiÃ¡ cao, giÃ y xáº¥u, shop giao hÃ ng cháº­m\"\n",
    "]\n",
    "\n",
    "\n",
    "print(\"=== CNN Predictions ===\")\n",
    "print(predict_aspects_cnn(cnn_models, cnn_encoders, sample_texts, vocab, seq_len=SEQ_LEN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fdc359dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All existing models and encoders saved to disk\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save models\n",
    "for aspect, model in cnn_models.items():\n",
    "    model_path = os.path.join(MODEL_DIR, f\"{aspect}_cnn_model.pt\")\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "\n",
    "# Save encoders (joblib is fine for these)\n",
    "import joblib\n",
    "for aspect, encoder in cnn_encoders.items():\n",
    "    joblib.dump(encoder, os.path.join(MODEL_DIR, f\"{aspect}_cnn_encoder.pkl\"))\n",
    "\n",
    "print(\"âœ… All existing models and encoders saved to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a29beba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# ----------------------\n",
    "# UDF factory for each aspect\n",
    "# ----------------------\n",
    "\n",
    "def make_predict_udf(aspect, seq_len=50):\n",
    "    model_path = os.path.join(MODEL_DIR, f\"{aspect}_cnn_model.pt\")\n",
    "    encoder_path = os.path.join(MODEL_DIR, f\"{aspect}_cnn_encoder.pkl\")\n",
    "\n",
    "     # Lazy load â€” only once per executor\n",
    "    model = None\n",
    "    encoder = None\n",
    "    vocab = build_vocab(train_df[TEXT_COL])\n",
    "\n",
    "    @pandas_udf(StringType())\n",
    "    def predict_udf(texts: pd.Series) -> pd.Series:\n",
    "        nonlocal model, encoder, vocab\n",
    "        if model is None:\n",
    "            import torch\n",
    "            import joblib\n",
    "\n",
    "            encoder = joblib.load(encoder_path)\n",
    "            model = CNNTextClassifier(vocab_size=len(vocab), embed_dim=EMBEDDING_DIM, seq_len=seq_len, num_classes=len(encoder.classes_))\n",
    "            state_dict = torch.load(model_path, map_location=\"cpu\")\n",
    "            model.load_state_dict(state_dict)\n",
    "            model.eval()\n",
    "\n",
    "        texts_proc = [preprocess_text(t) for t in texts]\n",
    "        X_seq = texts_to_sequences(texts_proc, vocab, seq_len)  # torch.tensor\n",
    "        with torch.no_grad():\n",
    "            logits = model(X_seq)\n",
    "            preds = torch.argmax(logits, dim=1).numpy()\n",
    "        preds_original = encoder.inverse_transform(preds)\n",
    "        return pd.Series([SENT_ID2NAME[int(x)] for x in preds_original])\n",
    "\n",
    "    return predict_udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7b7a6d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-------------+------------+------------+---------+-----------------+------------+-----------+\n",
      "|              Review|Price_pred|Shipping_pred|Outlook_pred|Quality_pred|Size_pred|Shop_Service_pred|General_pred|Others_pred|\n",
      "+--------------------+----------+-------------+------------+------------+---------+-----------------+------------+-----------+\n",
      "|GiÃ y hÆ¡i cÃ³ mÃ¹i n...|      None|         None|        None|    Negative|     None|             None|        None|       None|\n",
      "|HÃ ng vá» Ä‘áº¹p láº¯m n...|      None|     Positive|    Positive|        None| Positive|             None|        None|       None|\n",
      "|HÃ ng Ã´k nÃªn mua D...|      None|         None|    Positive|        None|     None|             None|    Positive|       None|\n",
      "|Bun. GTI gá»­i Oke ...|      None|         None|        None|        None|     None|             None|        None|       None|\n",
      "|MÃ u Ä‘áº¹p giá»‘ng tro...|      None|         None|    Positive|        None|     None|             None|        None|       None|\n",
      "|cháº¥t lÆ°á»£ng phÃ¹ há»£...|      None|         None|        None|     Neutral|  Neutral|             None|        None|       None|\n",
      "|GiÃ y trÆ°á»£t láº¯m hu...|      None|         None|        None|        None|     None|             None|        None|       None|\n",
      "|Tr Æ¡i dÃ©p Ä‘áº¹p vs ...|      None|         None|    Positive|    Positive| Positive|             None|        None|       None|\n",
      "|  CÅ©ng táº¡m Ä‘Æ°á»£c thoi|      None|         None|        None|        None|     None|             None|     Neutral|       None|\n",
      "|Shop há»— trá»£ ráº¥t t...|      None|         None|        None|        None|     None|         Positive|    Positive|       None|\n",
      "+--------------------+----------+-------------+------------+------------+---------+-----------------+------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Load some Spark dataframe (example: test data)\n",
    "spark_df = spark.read.csv(TEST_PATH, header=True)\n",
    "df = spark_df.limit(10)\n",
    "\n",
    "# Add predictions for each aspect\n",
    "for aspect in ASPECTS:\n",
    "    predict_udf = make_predict_udf(aspect)\n",
    "    df = df.withColumn(f\"{aspect}_pred\", predict_udf(df[\"Review\"]))  # check column name\n",
    "\n",
    "# Show results\n",
    "cols_to_show = [\"Review\"] + [f\"{aspect}_pred\" for aspect in ASPECTS]\n",
    "df.select(*cols_to_show).limit(10).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
